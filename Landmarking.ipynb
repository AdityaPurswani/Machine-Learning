{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Landmarking.ipynb","provenance":[],"authorship_tag":"ABX9TyNX0NhoO5wSdo1oHWcRLqjb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"bgcbse4Mtw-a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1635761046568,"user_tz":-330,"elapsed":11454,"user":{"displayName":"Aditya Purswani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlsqkbzme4r_DNKVgMAuIwxR0eLbLbhO89oyhf=s64","userId":"11563067018386745270"}},"outputId":"f36dd894-027c-4682-a382-e5dadfe01783"},"source":["!git clone https://github.com/codeforcauseorg/ML-Bootcamp-July"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'ML-Bootcamp-July'...\n","remote: Enumerating objects: 222, done.\u001b[K\n","remote: Counting objects: 100% (33/33), done.\u001b[K\n","remote: Compressing objects: 100% (26/26), done.\u001b[K\n","remote: Total 222 (delta 12), reused 24 (delta 5), pack-reused 189\u001b[K\n","Receiving objects: 100% (222/222), 119.10 MiB | 24.82 MiB/s, done.\n","Resolving deltas: 100% (55/55), done.\n","Checking out files: 100% (131/131), done.\n"]}]},{"cell_type":"code","metadata":{"id":"wSUQtfZltiDw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"error","timestamp":1635761739157,"user_tz":-330,"elapsed":1901,"user":{"displayName":"Aditya Purswani","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghlsqkbzme4r_DNKVgMAuIwxR0eLbLbhO89oyhf=s64","userId":"11563067018386745270"}},"outputId":"167afcc0-9b73-4527-a840-fa6990b85179"},"source":["import cv2\n","import dlib\n","import numpy as np\n","import os\n","\n","cam = cv2.VideoCapture(0)\n","\n","detector = dlib.get_frontal_face_detector()\n","predictor = dlib.shape_predictor('/content/ML-Bootcamp-July/datasets/shape_predictor_68_face_landmarks.dat')\n","\n","mood = input('Enter your mood: ')\n","\n","frames = []\n","outputs = []\n","while True:\n","    ret, frame = cam.read()\n","\n","    cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n","    faces = detector(frame)\n","    for face in faces:\n","      landmarks = predictor(frame, face)\n","      #print(landmarks.parts)\n","      #nose = landmarks.parts()[27]\n","\n","      #lip_up = landmarks.parts()[62].y\n","     # lip_down =  landmarks.parts()[66].y\n","\n","      #if lip_down - lip_up > 5:\n","      #  print('Mouth open')\n","     # else:\n","       # print('Mouth Closed')\n","      #print(nose.x, nose.y)\n","      expression = np.array([[point.x - face.left(), point.y - face.top()] for point in landmarks.parts()[17:]])\n","      #print(expression.flatten())\n","\n","      #for point in landmarks.parts()[48:]:\n","       # cv2.circle(frame, (point.x, point.y), 2, (255, 0, 0), 2)\n","\n","\n","    #print(faces)\n","\n","    if ret:\n","        cv2.imshow(\"My frame\", frame)\n","\n","        if cv2.waitKey(1)==ord('q'):\n","            break\n","\n","        elif cv2.waitKey(5) == ord('c'):\n","          #cv2.imwrite(name + \".jpg\", frame)\n","          frames.append(expression.flatten())\n","          outputs.append([mood])\n","          \n","X = np.array(frames)\n","y = np.array(outputs)\n","\n","data = np.hstack([y, X])\n","\n","f_name = \"face_mood.npy\"\n","\n","# For file exits or not refer sr.py down\n","if os.path.exists(f_name):\n","    old_data = np.load(f_name)\n","    data = np.vstack([old_data, data])\n","\n","np.save(f_name, data)\n","cam.release()\n","cv2.destroyAllWindows()\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-585d125b34dd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mfaces\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mface\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m       \u001b[0mlandmarks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mface\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __call__(): incompatible function arguments. The following argument types are supported:\n    1. (self: dlib.fhog_object_detector, image: array, upsample_num_times: int=0) -> dlib.rectangles\n\nInvoked with: <dlib.fhog_object_detector object at 0x7fb09957f0f0>, None"]}]},{"cell_type":"code","metadata":{"id":"qzRqaAs2983S"},"source":[""],"execution_count":null,"outputs":[]}]}